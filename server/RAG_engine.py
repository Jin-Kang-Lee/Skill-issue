import pandas as pd
import faiss
import numpy as np
import os
import requests
from tqdm import tqdm
import json 

# === Load cleaned resume dataset from CSV ===
df = pd.read_csv("media/Resume.csv")
texts = df['Resume_str'].tolist()

# === Paths to store/load FAISS vector index and associated metadata ===
INDEX_PATH = "vector_store/faiss.index"
META_PATH = "vector_store/metadata.npy"

# === Embedding cache to avoid redundant API calls ===
embedding_cache = {}

def get_ollama_embedding(text, model="nomic-embed-text"):
    response = requests.post(
        "http://localhost:11434/api/generate",  # âœ… Use /api/generate for general-purpose models
        json={
            "model": model,
            "prompt": f"Return only the embedding vector of this text:\n{text}",
            "options": {"embedding_only": True}
        }
    )
    response.raise_for_status()
    embedding = response.json()["embedding"]
    return embedding

def build_or_load_index():
    """
    Loads an existing FAISS index and metadata if available.
    If not, generates embeddings using Ollama and builds the FAISS index.
    Saves both the index and metadata to disk for future use.
    """
    if os.path.exists(INDEX_PATH) and os.path.exists(META_PATH):
        # Load index and metadata from disk
        index = faiss.read_index(INDEX_PATH)
        metadata = np.load(META_PATH, allow_pickle=True)
        return index, metadata
    else:
        # Generate embeddings for each resume and build the index
        print("Generating embeddings using Ollama...")
        embeddings = [get_ollama_embedding(text) for text in tqdm(texts)]
        embeddings = np.array(embeddings).astype("float32")

        # Create a FAISS index using L2 (Euclidean distance)
        index = faiss.IndexFlatL2(embeddings.shape[1])
        index.add(embeddings)

        # Save the index and metadata for reuse
        os.makedirs("vector_store", exist_ok=True)
        faiss.write_index(index, INDEX_PATH)
        np.save(META_PATH, np.array(texts))

        return index, texts

# Load or build the index and corresponding resume metadata
index, metadata = build_or_load_index()

def retrieve_context(query, top_k=3):
    """
    Retrieves the top_k most similar resume entries to the query.
    Converts the query to an embedding and performs FAISS nearest neighbor search.
    Returns the top-k matching resume texts.
    """
    query_vec = np.array([get_ollama_embedding(query)]).astype("float32")
    D, I = index.search(query_vec, top_k)  # D = distances, I = indices
    return [metadata[i] for i in I[0]]

def generate_answer(question, context_chunks, model="phi3:mini"):
    """
    Sends the question along with resume context to Ollama for LLM-based response generation.
    Streams the output line-by-line to handle partial responses and errors.
    Returns a complete answer string generated by the LLM.
    """
    # Format the context and question into a single prompt
    context = "\n---\n".join(context_chunks)
    prompt = f"""
    You are ResumeBot. You are given context from candidate resumes below.

    Context:
    {context}

    Answer the following question using only the information above:

    Question: {question}
    """

    # Prepare request payload for the LLM API
    payload = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ]
    }

    try:
        # Make a streaming POST request to the LLM API
        response = requests.post("http://localhost:11434/api/chat", json=payload, stream=True)
        response.raise_for_status()

        # Accumulate response from streamed JSON lines
        full_response = ""
        for line in response.iter_lines():
            if not line:
                continue
            try:
                data = json.loads(line.decode('utf-8'))
                if 'message' in data and 'content' in data['message']:
                    full_response += data['message']['content']
            except json.JSONDecodeError:
                continue  # Skip malformed lines

        return full_response.strip() or "No content generated."
    except Exception as e:
        return f"Error communicating with Ollama: {str(e)}"
